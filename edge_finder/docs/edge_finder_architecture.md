# Edge Finder -- Architecture Document

## 1. Problem Statement

Mathlib is a large, continuously growing library of formalized mathematics in Lean 4.
Within it, there are regions where:

- Primitives exist but no unifying abstraction ties them together.
- Lemmas are scattered across namespaces with no dedicated module.
- Intermediate structural layers are missing between low-level definitions and high-level results.

These "pockets" represent opportunities for mini-library contributions.

The problem is: **how do you find them systematically, and how do you prove a pocket is genuinely underdeveloped rather than just unfamiliar to the searcher?**

The Edge Finder is a Python CLI pipeline that generates hypotheses about what *should* exist in Mathlib for a given theme, searches Mathlib via the LeanExplore API, analyzes the structural density of what it finds, and classifies potential gaps.

---

## 2. Goals and Non-Goals

### Goals

- Given a mathematical theme, produce an evidence-backed assessment of Mathlib's structural coverage.
- Distinguish between "not found by our search" and "genuinely absent" (the hallucinated-gap problem).
- Classify the *type* of gap (missing operator, namespace fragmentation, middle-layer gap, missing thematic file).
- Produce inspectable, logged artifacts so a human can audit every search query and its results.
- Run as a self-contained Python CLI with no interactive dependencies.

### Non-Goals (at this stage)

- **Library blueprint generation.** Proposing specific lemma lists, dependency DAGs, or API designs for a mini-library. The old Module 4 (Pocket Synthesis) attempted this prematurely. Blueprint generation belongs in a separate downstream subsystem that consumes the Edge Finder's output.
- **Automatic theorem generation or proof writing.**
- **Lean LSP integration.** Deferred to Phase B per the original outline.
- **Real-time or interactive operation.** This is a batch pipeline.

---

## 3. Current Component Breakdown

The system maps to the four-module architecture from `edge_finder_outline.md`, but with significant gaps in Modules 2 and 3.

```
                  +---------------------+
                  |    CLI Orchestrator  |
                  |      (cli.py)       |
                  +----------+----------+
                             |
         +-------------------+-------------------+
         |           |               |           |
  +------v-----+ +--v-----------+ +-v--------+ +v-----------+
  |  Module 1  | |  Module 2    | | Module 3 | |  Module 4  |
  |  Concept   | |  Search +    | | Density  | |  Pocket    |
  |  Generator | |  Verification| | + Gaps   | |  Candidate |
  +------------+ +--------------+ +----------+ |  Ranking   |
        |              |               |        +------------+
   OpenAI API    LeanExplore API   (local)       (local)
```

### Module 1: Concept Generator (implemented, improved in Iteration 5)

**Files:** `llm/concept_generator.py` (14 lines), `prompts.py:14-44`

**What it does:** Sends a theme string to OpenAI and receives structured JSON containing primitives, operators, identity families, candidate namespaces, and search queries. This is the "what should exist" hypothesis layer.

**Assessment:** Functional. The prompt was significantly improved in Iteration 5 with explicit Lean 4 / Mathlib naming convention guidance (CamelCase namespaces, dot-qualified snake_case lemmas, concrete examples of correct vs. incorrect names). Output quality depends on the LLM, but the naming conventions produce much better alignment with actual Mathlib names. One concern: the search queries are generated by the LLM with no grounding -- they could be poorly targeted. But this is acceptable for Phase A.

### Module 2: Verification Layer (implemented, two-pass + concept re-mapping as of Iteration 5)

**Files:** `search/leanexplore.py` (~155 lines), `search/models.py` (113 lines), `search/logger.py` (45 lines), `verification/__init__.py` (~468 lines), `llm/concept_remapper.py` (~92 lines)

**What the outline requires:**
- Check whether generated objects/abstractions actually exist in Mathlib.
- Use: Lean LSP, semantic search, grep fallback, namespace inspection.
- Output verification statuses: `verified exists` / `partially exists` / `does not exist` / `exists under different naming`.
- Never trust LLM claims without search confirmation.

**What is actually implemented (two-pass verification with concept re-mapping):**

**Concept re-mapping (Iteration 5):** After theme-level search, the LLM is asked to re-map generated concept names to actual Mathlib declaration names observed in the search results. This addresses the naming convention mismatch where the LLM generates textbook-style names but Mathlib uses qualified Lean 4 names. The re-mapping prompt receives the original concept lists and a flat list of unique Mathlib names from search results (up to 2000 names). It returns the same structure with names replaced by their best match (or null if genuinely absent). Null values fall back to the original name. File: `llm/concept_remapper.py`.

**Pass 1 (theme-level, Iteration 1):** Semantic search via LeanExplore API using the theme-level queries from the Concept Generator. The `verify_concepts()` function extracts each primitive, operator, and identity family from the (optionally remapped) concepts JSON and classifies it against all search results using name-based heuristics:
  - `verified`: exact name match or qualified suffix (e.g., `Finset.sum` matches concept `sum`)
  - `partial`: concept appears as substring in a result name
  - `absent`: no results match by any criterion
  - `name_mismatch`: concept appears in informalization/docstring but not in the result name

**Pass 2 (targeted, Iteration 2):** For each concept classified as `absent` or `partial` after pass 1, `refine_verification()` generates targeted search queries (the concept name itself, namespace-prefixed variants, space-separated variants) and searches LeanExplore. Re-classifies using the combined original + targeted results. Error handling ensures a single query failure (e.g., LeanExplore 500 error) does not crash the pipeline.

**Search layer resilience (Iteration 5):** Both `search_single()` and `search_queries()` now include retry logic with exponential backoff (up to 2 retries) for transient HTTP 5xx errors. `search_queries()` handles errors per-query instead of aborting the entire batch. This was added after Iteration 5 calibration revealed 131/131 targeted queries failing due to transient LeanExplore server issues.

The VerificationReport provides a summary (total/verified/partial/absent/name_mismatch counts, coverage ratio) and per-entry detail with match evidence. Each entry records which queries were searched (both theme-level and targeted).

Output artifacts: `verification.json`, `concepts_remapped.json`, `remap_log.json` in each run directory.

**What is still missing:**
1. **Lean LSP search.** Deferred; empirical results from Iteration 2 suggest LeanExplore targeted queries are sufficient for most concepts.
2. **Grep fallback.** Not implemented. Could be done against a local Mathlib checkout.
3. **Namespace inspection.** Not implemented. `_namespace_of()` in `density.py` extracts only the first dot-separated segment.
4. **Concept name normalization beyond re-mapping.** Iteration 5 added LLM-based concept re-mapping, which addresses the naming convention mismatch for ~80% of changed names. The remaining ~20% of remappings are inaccurate (mapping to semantically adjacent but wrong declarations). A stronger model or post-hoc validation step could improve this.

**Note on Lean LSP (Phase B):** The Iteration 2 test run achieved 100% coverage ratio (11 verified, 1 partial, 0 absent) on the "finite sums" theme with only 16 targeted API calls. This suggests that per-concept targeted queries via LeanExplore may be sufficient for Phase A, and LSP integration can be deferred further.

### Module 3: Density & Structure Analyzer (redesigned in Iteration 4)

**Files:** `analysis/density.py` (179 lines), `analysis/heuristics.py` (218 lines)

The density analyzer computes aggregate structural metrics from search results. The gap heuristics classifier uses **verification coverage as its sole gap signal** and reports density metrics as informational context only.

**Iteration 3 improvements (density analyzer):**

- **`_namespace_of()` uses 2-level namespaces.** `Finset.sum.comm` maps to `Finset.sum` instead of `Finset`.
- **`_operator_candidates()` expanded to 28 suffix patterns** covering aggregation, functorial, algebraic, lattice, set-theoretic, and composition operators.

**Iteration 4 redesign (gap classifier):**

Calibration against 6 themes (3 known-strong, 3 known-sparse) showed that density-derived heuristics (`missing_operator_abstraction`, `namespace_fragmentation`, `middle_layer_gap`, `missing_thematic_file`) fired identically on all themes because LeanExplore returns a fixed-size result set per query. Density metrics have zero discriminative power.

The classifier now produces:
- **`concept_coverage_gap`:** Sole gap signal. Tiered confidence calibrated from empirical data: `high` (coverage < 0.3), `moderate` (coverage < 0.5), `low` (coverage < 0.7). No gap finding if coverage >= 0.7.
- **Search coverage modifier:** If < 70% of queries returned results, confidence is downgraded one tier (API failures make absence unreliable).
- **`density_context`:** Informational summary of density metrics (always present, not a gap claim).
- **`well_covered`:** Positive finding when coverage >= 0.7 (no gaps detected).
- **`insufficient_signal`:** Fallback for zero declarations or no verification data.

**Remaining weaknesses:**

- **No thematic file detection.** Would require local Mathlib checkout or additional API queries.
- **Coverage thresholds calibrated on only 6 themes.** May need refinement with more diverse themes.

### Module 4: Pocket Candidate Ranking (designed, not yet implemented)

**Files:** `analysis/candidates.py` (planned, ~150-200 lines)

**Design doc:** `candidate_ranker/edge_finder_pocket_candidates.md`

**What it does:** Takes the gap evidence from Modules 1-3 (VerificationReport, DensityReport, GapReport, raw search responses) and produces a ranked list of pocket candidates -- clusters of related absent/partial concepts that together represent coherent, feasible mini-library contribution opportunities.

**Key design decisions:**
- **Purely algorithmic, no LLM call.** Deterministic and reproducible. Avoids the "confident nonsense" problem of the old pocket synthesis.
- **Namespace-prefix clustering.** Groups under-covered concepts by shared namespace prefix.
- **Three-factor scoring:** cluster size (0.30), infrastructure proximity (0.35), concept coherence (0.35).
- **Minimum cluster size of 2.** Single missing concepts go to `unclustered_concepts`.
- **Output: `candidates.json`** with ranked candidates, score breakdowns, justifications, risk assessment.

**What it replaces:** The old Module 4 (Pocket Synthesis) asked an LLM to generate a mini-library blueprint from aggregate metrics and raw search results. That module was premature -- it produced plausible-sounding but ungrounded output because it required mathematical judgment the LLM could not reliably provide. The code (`llm/pocket_synthesis.py`, `prompts.py:54-80`) is retained in the codebase but removed from the pipeline scope. Blueprint generation belongs in a separate downstream subsystem.

---

## 4. Data Flow

```
Theme (string)
    |
    v
[Concept Generator] -- OpenAI API --> concepts.json
    |                                    Contains: primitives, operators,
    |                                    identity_families, candidate_namespaces,
    |                                    search_queries
    v
[LeanExplore Search] -- LeanExplore API --> SearchResponse[] (theme-level)
    |                                         One per query, each containing
    |                                         SearchResult[] with name, module,
    |                                         docstring, source_text, dependencies
    v
[Concept Re-Mapping] -- OpenAI API --> concepts_remapped.json + remap_log.json
    |  (Iteration 5)                     Maps LLM-generated concept names to
    |                                    actual Mathlib declaration names observed
    |                                    in search results. Null = genuinely absent.
    |                                    (skippable with --skip-remap)
    v
[Verification Pass 1] --> initial VerificationReport
    |                       Matches (remapped) concepts against theme-level
    |                       results using name-based heuristics
    v
[Verification Pass 2] -- LeanExplore API --> SearchResponse[] (targeted)
    |                       For absent/partial concepts: generates targeted
    |                       queries, searches, re-classifies
    |                       --> final VerificationReport --> verification.json
    v
[Density Analyzer] --> DensityReport
    |                    Consumes ALL responses (theme + targeted)
    |                    Contains: namespace stats, module list,
    |                    operator hits, spread ratios, namespace depths
    v
 [Gap Heuristics] --> GapReport
     |                  Consumes: DensityReport + VerificationReport
     |                  + SearchResponse[] (for search coverage)
     |                  Contains: GapFinding[] with category,
     |                  confidence, evidence strings
    v
[Pocket Candidate Ranking] --> candidates.json
    |                            Consumes: VerificationReport +
    |                            DensityReport + GapReport +
    |                            concepts dict + SearchResponse[]
    |                            Contains: ranked pocket candidates
    |                            with score breakdowns, justifications,
    |                            risk assessment
    v
[Output Writer] --> run_<timestamp>_<theme>/
                      concepts.json
                      concepts_remapped.json (Iteration 5)
                      remap_log.json (Iteration 5)
                      search_log.jsonl (theme + targeted queries)
                      search_responses.json (theme queries only)
                      verification.json
                      candidates.json
                      run_<date>_<theme>.json (full report)
```

### Key observation about the data flow

As of Iteration 5, the data flow has two LLM-assisted steps before verification: concept generation (Module 1) and concept re-mapping. The re-mapping step sits between theme-level search and verification, turning the LLM from a name-guesser into a name-matcher (an easier task). The gap heuristics use **verification coverage as the sole gap signal**. Density metrics are computed and reported for human inspection but do not produce gap claims. The gap classifier consumes three inputs: the `DensityReport` (informational context), the `VerificationReport` (gap detection), and the raw `SearchResponse[]` list (search coverage computation for confidence adjustment). The density analyzer and gap heuristics receive `all_responses` (theme + targeted) rather than theme-only responses.

The new Module 4 (Pocket Candidate Ranking) operates purely on existing pipeline data -- no additional LLM or API calls. It clusters absent/partial concepts from the VerificationReport by namespace prefix, scores each cluster on size, infrastructure proximity, and coherence, and outputs a ranked list of pocket candidates. This replaces the old Pocket Synthesis module, which used an LLM call to generate blueprints from unverified evidence.

---

## 5. Key Design Decisions Made

| Decision | Rationale | Tradeoff |
|---|---|---|
| Self-contained Python CLI, no OpenCode in pipeline | Reproducibility, scriptability, no interactive dependency | Cannot leverage OpenCode's conversational search |
| OpenAI API (`gpt-4o-mini` default) | Cost-effective, fast, sufficient for structured prompts | Less capable than `gpt-4o` for nuanced mathematical reasoning |
| LeanExplore API as sole search mechanism | Available, semantic search, Mathlib-specific | No fallback if queries miss; no namespace enumeration |
| Frozen dataclasses for all models | Immutability, hashability, predictability | Slightly more verbose than dicts |
| JSONL search log | Append-only, one entry per query, easy to audit | Not queryable without parsing |
| Evidence summarization in prompts.py | Context window management for gpt-4o-mini | Lossy -- strips source_text, truncates docstrings |

---

## 6. Alternatives Considered

- **OpenCode-driven pipeline** (agent uses MCP tools interactively): Rejected because it is non-reproducible and requires human-in-the-loop during execution.
- **Local Mathlib grep** instead of LeanExplore API: Not yet implemented but identified as a necessary fallback.
- **Multiple LLM calls per concept** (verify each individually): Not implemented. Would strengthen verification but increase cost and latency.
- **LLM-based pocket assessment** (old Module 4 approach): Rejected. The LLM receives aggregate metrics and generates a blueprint with proposed abstractions and lemma lists. Output sounds plausible but is not grounded in confirmed evidence. Replaced with algorithmic candidate ranking.

---

## 7. Known Risks and Failure Modes

### R1: Hallucinated absence (critical, partially mitigated)
The system may report a gap that does not actually exist because the LLM generated poor search queries or LeanExplore's semantic search missed relevant results. The outline explicitly warns: "must separate 'not found in current search' from 'confirmed absent'." **Mitigations added:** Iteration 2 introduced per-concept targeted queries that significantly reduce false absences for concepts that exist under qualified names. Iteration 5 added concept re-mapping, which aligns generated names to actual Mathlib declaration names before verification. Together these raised average coverage from 0.33 to 0.60 across 6 calibration themes. However, the risk is not eliminated: concepts that genuinely don't appear in LeanExplore results (e.g., due to sparse documentation or non-standard naming) will still be classified as absent.

### R2: Aggregate metrics hide structure
Namespace counts and spread ratios treat all declarations as interchangeable. A namespace with 50 trivial lemmas and one with 50 deep theorems produce the same metrics. As of Iteration 4, density metrics are informational-only and do not produce gap claims, reducing the impact of this limitation.

### R3: Concept-to-evidence disconnect (largely mitigated)
The Concept Generator produces hypotheses. The search runs queries. Iteration 1 added per-concept verification that links each hypothesis to specific search results. Iteration 2 added per-concept targeted queries that directly search for each under-covered concept. Iteration 5 added concept re-mapping that aligns generated names to actual Mathlib names before verification. The remaining gap: concepts whose Mathlib declaration names are significantly different from any name the LLM or remapper can produce will still be missed.

### R4: Operator detection may still miss patterns
Expanded from 5 to 28 suffix patterns in Iteration 3, covering aggregation, functorial, algebraic, lattice, set-theoretic, and composition operators. However, suffix matching is inherently limited -- Mathlib operators that don't follow common naming patterns will be missed.

### R5: Pocket candidate ranking may not capture mathematical value
The algorithmic scoring (cluster size, infrastructure proximity, concept coherence) captures structural properties but cannot assess the mathematical significance or novelty of a pocket candidate. A highly-scored pocket may be structurally coherent but mathematically trivial. The human reviewer must apply mathematical judgment to the ranked output. This is a deliberate tradeoff: deterministic, inspectable scoring over LLM-generated "confident nonsense."

### R6: No test coverage
Zero tests exist. Any refactoring risks silent regressions.

---

## 8. Gap Analysis: Outline vs. Implementation

| Outline Requirement | Status | Notes |
|---|---|---|
| Concept Generator produces structured hypotheses | Done | Adequate for Phase A |
| Verification Layer uses Lean LSP | Not done | Deferred to Phase B |
| Verification Layer uses semantic search | Done | LeanExplore API |
| Verification Layer uses grep fallback | Not done | Needs local Mathlib checkout |
| Verification Layer uses namespace inspection | Not done | `_namespace_of()` is too coarse |
| Per-concept verification status (verified/partial/absent/name-mismatch) | Done | Two-pass: theme-level (Iter 1) + targeted queries (Iter 2). See `verification/__init__.py`. |
| Density analyzer: lemmas per namespace | Done | 2-level namespace extraction (Iteration 3) |
| Density analyzer: presence of thematic files | Not done | |
| Density analyzer: existence of operator definitions | Done | 28 suffix patterns (Iteration 3) |
| Density analyzer: coherence of naming patterns | Not done | |
| Density analyzer: dependency centrality | Not done | |
| Gap heuristics: middle-layer gap | Removed (Iter 4) | Density-derived; zero discriminative power across themes |
| Gap heuristics: missing operator abstraction | Removed (Iter 4) | Density-derived; zero discriminative power across themes |
| Gap heuristics: namespace fragmentation | Removed (Iter 4) | Density-derived; zero discriminative power across themes |
| Gap heuristics: lack of thematic file | Removed (Iter 4) | Density-derived; zero discriminative power across themes |
| Gap heuristics: high manual proof friction | Not done | |
| Pocket synthesis with justification from verified evidence | Replaced | Old Module 4 (LLM-based pocket synthesis) removed from scope. New Module 4 (algorithmic pocket candidate ranking) designed but not yet implemented. See `candidate_ranker/edge_finder_pocket_candidates.md`. |
| All search queries logged | Done | JSONL log includes both theme-level and targeted queries |
| All namespace scans logged | Not done | No namespace scans performed |
| Inspectable reasoning trace | Mostly done | Per-concept trace via verification.json; gap findings include verification context (Iteration 3) |
| Risk classification | Redesigned | Old Module 4 output included a risk_level. New Module 4 design includes per-candidate risk assessment based on gap confidence and infrastructure proximity. |
| Evaluation criteria scoring | Not done | |

---

## 9. Critical Invariants

1. **Every search query must be logged.** The system must never silently discard search results. (Currently maintained.)
2. **No gap claim without verification evidence.** The system must never assert something is missing from Mathlib without per-concept verification data. (Fully addressed as of Iteration 4: `concept_coverage_gap` is the sole gap signal and requires a `VerificationReport`. Density-derived heuristics were removed because they lacked discriminative power. Density metrics are reported as `density_context` for human inspection only.)
3. **Concepts and evidence must be linkable.** For each concept the generator proposes, there must be a traceable path to the search results that confirm or deny its presence. (Fully addressed by Iterations 1+2: `VerificationReport` provides this link, and targeted queries ensure each under-covered concept has been directly searched.)
4. **Output must be deterministic given the same inputs.** The LLM calls introduce non-determinism; this is acceptable but must be noted. Dry-run mode with cached concepts provides partial determinism.

---

## 10. Iteration Plan

### Iteration 1: Per-Concept Verification -- COMPLETE

**Goal:** Close the concept-to-evidence disconnect.

**What was built:** `verification/__init__.py` (~320 lines). For each primitive, operator, and identity family in the concepts JSON, the module matches against all search results using name-based heuristics (exact, qualified_suffix, substring, informalization) and classifies each concept as `verified`, `partial`, `absent`, or `name_mismatch`. Produces a `VerificationReport` with per-entry evidence and summary statistics. Output artifact: `verification.json` in each run directory.

**Pipeline integration:** `cli.py` calls `verify_concepts()` after search logging and before density analysis. The `verification_report` is included in the evidence dict passed to pocket synthesis and in the run report.

**What was learned from the test run ("finite sums" theme, 12 concepts):**

- 6 verified, 2 partial, 4 absent, 0 name_mismatch. Coverage ratio: 0.667.
- The 4 absent concepts (`sum_add_distrib`, `sum_mul`, `sum_sigma`, `Finset.map`) likely exist in Mathlib under qualified names, but were not surfaced because the broad theme-level search queries did not target them specifically.
- **Key insight:** Verification accuracy is bounded by search coverage. The broad queries are good at surfacing well-known primitives (`Finset`, `Fintype`, `Finset.sum`) but miss specific identity families. Per-concept targeted queries (Iteration 2) would significantly reduce false absences.
- The `name_mismatch` category was not triggered in this test. It would require a concept whose name differs from the Mathlib declaration but whose description matches. This may be more common for themes outside the user's mathematical domain.

### Iteration 2: Per-Concept Targeted Queries -- COMPLETE

**Goal:** Reduce false absences by searching specifically for each concept.

**What was built:** Added `generate_targeted_queries()` and `refine_verification()` to `verification/__init__.py` (~120 new lines). Added `search_single()` to `search/leanexplore.py` with error handling for API failures. Added `--skip-targeted-search` CLI flag.

**How it works:**
1. After pass 1 (theme-level verification), concepts classified as `absent` or `partial` are selected for refinement.
2. For each, `generate_targeted_queries()` produces 1-4 deterministic queries: the bare concept name, namespace-prefixed variants (top-level namespaces only, no self-prefixing), and a space-separated variant for semantic search.
3. Each query is searched via LeanExplore. Failed queries (HTTP errors) return empty results rather than crashing.
4. Matches from targeted results are merged with original matches, and the concept is re-classified.
5. Targeted search responses are logged to `search_log.jsonl` and included in `all_responses` for density analysis.

**Test results ("finite sums" theme, 12 concepts):**

| Metric | Pass 1 only | Pass 1 + Pass 2 |
|---|---|---|
| Verified | 6 | 11 |
| Partial | 2 | 1 |
| Absent | 4 | 0 |
| Coverage ratio | 0.667 | 1.0 |
| Targeted API calls | 0 | 16 |

All 4 previously-absent identity families were found by targeted queries:
- `sum_add_distrib` → verified (substring match: `finsum_add_distrib`)
- `sum_mul` → verified (substring match: `gaussSum_mul`)
- `sum_sigma` → verified (qualified_suffix: `Finset.sum_sigma`)
- `Finset.map` → verified (exact match)

The one remaining `partial` (`BigOperators`) is correct — it's a Lean import/namespace concept, not a declaration name.

**Key insights:**
- Targeted queries are highly effective. 16 API calls upgraded 5 concepts from absent/partial to verified.
- The query generation is conservative (1-4 queries per concept) and deterministic (no LLM needed).
- Error handling is important: the `sum_sigma` bare query returned a 500 error from LeanExplore, but the namespace-prefixed `Finset.sum_sigma` succeeded.
- For themes where the user's mathematical knowledge aligns with Mathlib naming conventions, the edge finder now achieves near-complete verification coverage.

### Iteration 3: Verification-Aware Density Analysis -- COMPLETE

**Goal:** Make density metrics more accurate and make gap classification verification-aware.

**What was built:**

1. **`density.py` -- `_namespace_of()` fixed** to use 2-level namespaces (first two dot-segments). `Finset.sum.comm` now maps to `Finset.sum` instead of `Finset`. This provides meaningful sub-namespace granularity without fragmenting into single-declaration buckets.

2. **`density.py` -- `_operator_candidates()` expanded** from 5 to 28 suffix patterns, organized into 6 categories: aggregation (`.sum`, `.prod`, `.fold`, `.foldl`, `.foldr`, `.reduce`), functorial (`.map`, `.bind`, `.pure`, `.filter`, `.filterMap`), algebraic (`.add`, `.mul`, `.sub`, `.div`, `.neg`, `.inv`, `.pow`), lattice (`.sup`, `.inf`, `.max`, `.min`), set-theoretic (`.union`, `.inter`, `.sdiff`, `.compl`), and composition (`.comp`, `.apply`).

3. **`heuristics.py` -- `classify_gaps()` now accepts optional `VerificationReport`.**
   - New `concept_coverage_gap` category: fires when `absent_count >= 2` or when `total >= 4 and coverage < 0.5`. Lists absent and partial concept names in evidence.
   - All density-derived findings now include verification context as a suffix in evidence strings (verified/partial/absent counts).
   - Backward compatible: `verification` parameter defaults to `None`.

4. **`cli.py` -- passes `verification_report` to `classify_gaps()`.**

5. **Dead files pruned:** `python/main.py` (empty) and `python/runner.py` (unused 9-line `lake build` wrapper) deleted.

**Test results ("finite sums" theme, 12 concepts):**

| Metric | Pass 1 only (`--skip-targeted-search`) | Pass 1 + Pass 2 (full) |
|---|---|---|
| Verification | 6 verified, 2 partial, 4 absent | 11 verified, 1 partial, 0 absent |
| `concept_coverage_gap` | Fired (moderate): 4 absent, 2 partial | Not fired (coverage 1.0) |
| Unique namespaces (2-level) | 194 | 546 |
| Operator hits | 2 | 15 |
| Operator hit ratio | 0.008 | 0.0143 |
| `missing_operator_abstraction` | Fired | Fired |
| `namespace_fragmentation` | Fired | Fired |

**Key insights:**
- The `concept_coverage_gap` signal correctly distinguishes between pre-targeted-search (under-covered) and post-targeted-search (well-covered) runs.
- 2-level namespaces produce much richer structural information (194 vs. ~20 with 1-level for the same data).
- Expanded operator detection catches 7.5x more operators (15 vs. 2 in full run).
- `namespace_fragmentation` and `missing_operator_abstraction` still fire even in the full run. These are valid structural observations about the "finite sums" Mathlib region (high module spread, low operator-to-declaration ratio). However, Iteration 4 calibration showed these density-derived signals have zero discriminative power across themes, and they were subsequently removed as gap categories.

### Iteration 4: Heuristic Calibration -- COMPLETE

**Goal:** Replace arbitrary thresholds with empirically grounded ones.

**Method:** Ran the full pipeline against 6 contrastive themes -- 3 where Mathlib coverage is known to be strong, 3 where it's known to be sparse. Collected all metrics. Runs saved under `runs/calibration/`.

**Calibration data:**

| Theme | Expect | Verified | Partial | Absent | Coverage | Decls | Namespaces | Modules | Spread | OpHitRatio |
|---|---|---|---|---|---|---|---|---|---|---|
| natural number addition | strong | 9 | 0 | 7 | 0.5294 | 1500 | 469 | 206 | 0.137 | 0.002 |
| list operations | strong | 22 | 1 | 11 | 0.6765 | 1500 | 1069 | 259 | 0.173 | 0.016 |
| group homomorphisms | strong | 6 | 2 | 21 | 0.2581 | 1500 | 858 | 400 | 0.267 | 0.012 |
| combinatorial species | sparse | 4 | 3 | 23 | 0.2333 | 1500 | 585 | 360 | 0.240 | 0.009 |
| tropical geometry | sparse | 0 | 0 | 29 | 0.0 | 1500 | 354 | 212 | 0.141 | 0.005 |
| matroid theory | sparse | 5 | 6 | 24 | 0.3056 | 1500 | 722 | 227 | 0.151 | 0.004 |

**Critical finding:** `total_declarations` is always ~1500 because LeanExplore returns a fixed-size result set per query (20 results x ~75 queries). This makes all density metrics (spread ratio, operator hit ratio, namespace counts) nearly constant across themes. Density-derived heuristics have **zero discriminative power**.

**Only verification coverage** (`coverage_ratio`) separates strong from sparse themes:
- Strong themes average coverage ~0.49
- Sparse themes average coverage ~0.18

**Decision:** Adopt verification-only gap signals (Path A). Remove all density-derived gap categories (`missing_operator_abstraction`, `namespace_fragmentation`, `middle_layer_gap`, `missing_thematic_file`). Keep `DensityReport` as informational context. Add search coverage as a confidence modifier.

**What was built:**
- Rewrote `heuristics.py` (~219 lines): verification-only `concept_coverage_gap` with tiered confidence (high/moderate/low, calibrated from empirical thresholds), `density_context` as informational, `well_covered` positive finding, search coverage downgrade mechanism.
- Updated `cli.py` to pass `responses` parameter to `classify_gaps()`.

**Regression test results:**
- "finite sums" (full): coverage 1.0, `well_covered` fired, no gap (correct).
- "finite sums" (skip-targeted): coverage 0.667, `concept_coverage_gap (low)` fired (correct).
- "tropical geometry": coverage 0.0, `concept_coverage_gap (moderate)` fired (downgraded from high due to 24% search coverage -- 76% of queries returned 500 errors).

**Note:** "group homomorphisms" performed unexpectedly poorly (coverage 0.26 for a known-strong theme), suggesting LLM concept naming quality is a major variable. A concept name normalization step may be worth investigating.

### Iteration 5: Concept Name Grounding -- COMPLETE

**Goal:** Fix the naming convention mismatch where the LLM generates textbook-style concept names (e.g., `first_isomorphism_theorem`) but Mathlib uses qualified Lean 4 names (e.g., `QuotientGroup.quotientKerEquivRange`). This was identified as the dominant failure mode in Iteration 4 calibration (e.g., "group homomorphisms" scored 0.26 coverage despite being well-developed in Mathlib).

**Two-part approach:**

**Part 1: Improved concept generation prompt.** Added explicit Lean 4 / Mathlib naming convention guidance to `build_concept_generator_prompt()` in `prompts.py`: CamelCase namespaces/types, dot-qualified snake_case lemmas, concrete examples of correct vs. incorrect names. Prompt grew from 17 to 29 lines of instruction text.

**Part 2: LLM concept re-mapping.** New `llm/concept_remapper.py` (~92 lines). After theme-level search, extracts all unique declaration names from search results and asks the LLM to match each generated concept to the best Mathlib name (or null if genuinely absent). The re-mapping prompt (`build_concept_remap_prompt()` in `prompts.py`) receives the original concept lists and up to 2000 unique Mathlib names. Null values fall back to the original name (downstream consumers always see strings).

**Pipeline integration:**
- New `--skip-remap` CLI flag
- New pipeline step in `cli.py` after theme-level search, before pass 1 verification
- New output artifacts: `concepts_remapped.json` (remapped concepts dict), `remap_log.json` (mapping with reasons)
- Both `verify_concepts()` and `refine_verification()` operate on remapped concepts

**Search layer improvements:**
- Added retry logic with exponential backoff (2 retries, 1s/2s delays) to both `search_single()` and `search_queries()` for transient HTTP 5xx errors
- `search_queries()` now handles errors per-query instead of aborting the batch
- Fixed `"null"` string bug in `concept_remapper.py` where LLM returns string `"null"` instead of JSON `null`

**Calibration results (6 themes, Iteration 4 vs. Iteration 5):**

| Theme | Expect | Iter 4 Coverage | Iter 5 Coverage | Delta | Iter 5 Gap |
|---|---|---|---|---|---|
| natural number addition | strong | 0.5294 | 1.0000 | +0.47 | well_covered |
| list operations | strong | 0.6765 | 0.6786 | +0.00 | concept_coverage_gap |
| group homomorphisms | strong | 0.2581 | 0.7200 | +0.46 | well_covered |
| combinatorial species | sparse | 0.2333 | 0.2222 | -0.01 | concept_coverage_gap |
| tropical geometry | sparse | 0.0000 | 0.3043 | +0.30 | concept_coverage_gap |
| matroid theory | sparse | 0.3056 | 0.6667 | +0.36 | concept_coverage_gap |
| **Average** | | **0.3338** | **0.5986** | **+0.26** | |

Average coverage nearly doubled (0.33 -> 0.60). Gap classifications now correctly separate well-covered themes from sparse ones.

**Re-mapping effectiveness:** The remapper is most valuable for under-covered themes where concept names are furthest from actual declaration names (e.g., matroid theory: `Independence` -> `IndepMatroid`, `Circuit` -> `Matroid.IsCircuit`). For well-covered themes, the improved prompt (Part 1) does most of the work. Some remappings (~20% of changed names) are inaccurate (mapping to semantically adjacent but wrong declarations, e.g., `List.filter` -> `List.filterMap`). Net assessment: positive but not worth further tuning at this stage.

**Key insights:**
- The improved prompt (Part 1) contributes more to accuracy gains than the re-mapping (Part 2)
- Transient LeanExplore 500 errors were the dominant blocker in earlier runs; retry logic resolved this
- The `"null"` string bug caused nonsense targeted queries (e.g., `Subgroup.null`); now fixed
- Combinatorial species coverage was flat because the topic genuinely barely exists in Mathlib
- List operations coverage was already high pre-Iteration 5; naming conventions were less of an issue there

### Deferred: Lean LSP Integration (Phase B)

- Symbol lookup for exact name matching.
- Type signature inspection.
- Namespace enumeration.
- This would dramatically improve verification accuracy but requires significant infrastructure.
